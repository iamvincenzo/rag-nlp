"""
Script used to generate the user interface and to perform question-answering against your data,
using Atlas Vector Search and OpenAI.
"""

import gradio as gr
from gradio.themes.base import Base
from langchain.chains import RetrievalQA
from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch
from langchain_openai import OpenAIEmbeddings
from langchain_openai.llms import OpenAI
from pymongo import MongoClient

from key_param import MONGO_URI
from key_param import OPENAI_API_KEY


def query_data(query: str) -> tuple:
    """
    Function that performs semantic similarity search using Atlas Vector Search and
    uses a retrieval-based augmentation to perform question-answering on the data.

    Args:
        - query (str): A text query for semantic similarity search and question-answering.

    Returns:
        - tuple: A tuple containing the output from Atlas Vector Search and the output generated using RAG Architecture.
    """
    # convert question to vector using OpenAI embeddings
    # perform Atlas Vector Search using Langchain's vectorStore similarity_search
    # returns MongoDB documents most similar to the query
    docs = vector_store.similarity_search(query=query, K=1)

    if not docs:
        return "No relevant documents found.", "No result generated from RAG."

    as_output = docs[0].page_content
    # leveraging Atlas Vector Search paired with Langchain's QARetriever

    # Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model
    # if it's not specified (for example like in the code below),
    # then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023
    llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)

    # get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore
    # implements _get_relevant_documents which retrieves documents relevant to a query
    retriever = vector_store.as_retriever()

    # Load "stuff" documents chain. Stuff documents chain takes a list of documents,
    # inserts them all into a prompt and passes that prompt to an LLM
    qa = RetrievalQA.from_chain_type(llm, chain_type="stuff", retriever=retriever)

    # execute the chain
    retriever_output = qa.invoke(query)

    # return Atlas Vector Search output, and output generated using RAG Architecture
    return as_output, retriever_output["result"]


if __name__ == "__main__":
    # set the MongoDB URI
    client = MongoClient(MONGO_URI)
    # set the database
    dbName = "langchain_demo"
    # set the collection name
    collectionName = "collection_of_text_blobs"
    # create the collection
    collection = client[dbName][collectionName]

    # define the OpenAI Embedding Model we want to use for the source data
    # the embedding model is different from the language generation model
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

    # initialize the vector store
    vector_store = MongoDBAtlasVectorSearch(collection, embeddings)

    with gr.Blocks(theme=Base(), title="Question Answering App using Vector Search + RAG") as demo:
        gr.Markdown("""# Question Answering App using Atlas Vector Search + RAG Architecture""")
        textbox = gr.Textbox(label="Enter your Question:")
        with gr.Row():
            button = gr.Button("Submit", variant="primary")
        with gr.Column():
            output1 = gr.Textbox(lines=1, max_lines=10,
                                 label="Output with just Atlas Vector Search (returns text field as is):")
            output2 = gr.Textbox(lines=1, max_lines=10,
                                 label="Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA "
                                       "+ OpenAI LLM:")

        # call query_data function upon clicking the Submit button
        button.click(fn=query_data, inputs=[textbox], outputs=[output1, output2])

    demo.launch()
